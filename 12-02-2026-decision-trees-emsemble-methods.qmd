---
title: "Decision trees and Ensemble Methods"
author: "Ayush Patel"
subtitle: "DDD: Elements of Statistical Machine Learning & Politics of Data"
institute: "At Azim Premji University, Bhopal"
date: today
date-format: "DD MMM, YYYY"
format: 
  revealjs:
    fig-height: 6
    fig-width: 14
    embed-resources: true
    margin-left: 50px
    margin-right: 50px
    slide-number: c/t
    width: 1400
    height: 850
    theme: [default, theme.scss]
    footer: "Email: ayush.ap58@gmail.com"
---

```{r}
#| include: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(openintro)
library(ISLR2)
library(tree)
library(randomForest)
library(gbm)
library(BART)
library(palmerpenguins)
```

# Hello

::::{.columns}

:::{.column}

[I am Ayush.]{.fragment fragment-index="1" style="font-size:45px"}

[I am a researcher working at the intersection of data, development and economics.]{.fragment fragment-index="2" style="font-size:25px"}

[I am a [RStudio (Posit) certified tidyverse Instructor.](https://education.rstudio.com/trainers/people/patel+ayush/)]{.fragment fragment-index="3" style="font-size:25px"}

[I am a Researcher at [Oxford Poverty and Human development Initiative (OPHI)](https://ophi.org.uk/), at the University of Oxford.]{.fragment fragment-index="4" style="font-size:25px"}

:::

:::{.column}

![[Hello (Eh bonjour– donc–.) by Charles Motte](https://www.metmuseum.org/art/collection/search/392118"Well)](https://images.metmuseum.org/CRDImages/dp/original/DP808141.jpg){fig-align="center" height=400 .lightbox}
 

:::

::::

# Did you come prepared?

::::{.columns}

:::{.column}
- You have installed R. If not see [this link]().

- You have installed RStudio/Positron/VScode or any other IDE. It is recommended that you work through an IDE

- You have the libraries `{tree} {gbm} {randomForest} {BART}` installed

:::

:::{.column}

![[Armor (Gusoku) Helmet signed by Bamen Tomotsugu](https://www.metmuseum.org/art/collection/search/24975)](https://images.metmuseum.org/CRDImages/aa/original/DT5333.jpg){fig-align="center" height=400 .lightbox}


:::

::::


# Learning Goals

1. What are decision trees?
2. When to use them?
3. How do these work?
4. Application and interpretation.
4. Improving decision trees using ensemble methods.

# Decision Trees

<br>
[Supervised and non-parametric]{.fragment fragment-index='1'}
<br><br>
[Can be used for Classification and Regression]{.fragment fragment-index='2'}
<br><br>
[A predictor space is cut into segments and mean response of training observations is used as the estimate of response of test observations.]{.fragment fragment-index='3'}
<br><br>
[Simple, easy to interpret but not the *best* for prediction accuracy on its own]{.fragment fragment-index='4'}
<br><br>
[Prediction accuracy can be improved by ensemble methods]{.fragment fragment-index='5'}

# Intuition - How it works

```{r}
#| echo: false
#| fig-cap: Can you Identify regions with similar salary range?


Hitters |>
    ggplot(aes (y = Hits, x = Years)) +
    geom_point(aes(colour = Salary), size = 3) +
    guides(colour = guide_colourbar(barwidth = unit(15, "cm"), 
    barheight = unit(1, "cm")))+
    colorspace::scale_color_continuous_divergingx(
        palette = "RdYlBu",
        name = "Salary Quantiles",
        breaks = ~ quantile(.x, na.rm=T))+
    labs(
        y = "X1: Number of hits in the previous season",
        x = "X2: Number of years in the league",
        title = "Predictor Space"
    )+
    theme_minimal() +
    theme(
        plot.title.position = "plot",
        legend.position = "top",
        legend.title.position = "top",
        legend.direction = "horizontal"
    )

```

# Intuition - How it works

```{r}
#| echo: false
#| fig-cap: Can you Identify regions with similar salary range?


Hitters |>
    ggplot(aes (y = Hits, x = Years)) +
    geom_point(aes(colour = Salary), size = 3) +
    geom_vline(aes(xintercept = 4.8)) +
    guides(colour = guide_colourbar(barwidth = unit(15, "cm"), 
    barheight = unit(1, "cm")))+
    colorspace::scale_color_continuous_divergingx(
        palette = "RdYlBu",
        name = "Salary Quantiles",
        breaks = ~ quantile(.x, na.rm=T))+
    labs(
        y = "X1: Number of hits in the previous season",
        x = "X2: Number of years in the league",
        title = "Predictor Space"
    )+
    theme_minimal() +
    theme(
        plot.title.position = "plot",
        legend.position = "top",
        legend.title.position = "top",
        legend.direction = "horizontal"
    )

```

# Intuition - How it works

```{r}
#| echo: false
#| fig-cap: Can you Identify regions with similar salary range?


Hitters |>
    ggplot(aes (y = Hits, x = Years)) +
    geom_point(aes(colour = Salary), size = 3) +
    geom_vline(aes(xintercept = 4.8)) +
    annotate(geom = "segment",x = 4.8, y = 125, xend = 25, yend = 125) + 
    guides(colour = guide_colourbar(barwidth = unit(15, "cm"), 
    barheight = unit(1, "cm")))+
    colorspace::scale_color_continuous_divergingx(
        palette = "RdYlBu",
        name = "Salary Quantiles",
        breaks = ~ quantile(.x, na.rm=T))+
    labs(
        y = "X1: Number of hits in the previous season",
        x = "X2: Number of years in the league",
        title = "Predictor Space"
    )+
    theme_minimal() +
    theme(
        plot.title.position = "plot",
        legend.position = "top",
        legend.title.position = "top",
        legend.direction = "horizontal"
    )

```

# Intuition - How it works

```{r}
#| echo: false
#| fig-cap: Can you Identify regions with similar salary range?


Hitters |>
    ggplot(aes (y = Hits, x = Years)) +
    geom_point(aes(colour = Salary), size = 3) +
    geom_vline(aes(xintercept = 4.8)) +
    annotate(geom = "segment",x = 4.8, y = 125, xend = 25, yend = 125) + 
    annotate(geom = "text",x = 2.5, y = 125, 
    label = "R1", colour = "red", alpha = 0.3, size = 20) + 
    annotate(geom = "text",x = 15, y = 200, 
    label = "R3", colour = "red", alpha = 0.3, size = 20) + 
    annotate(geom = "text",x = 15, y = 50, 
    label = "R2", colour = "red", alpha = 0.3, size = 20) + 
    guides(colour = guide_colourbar(barwidth = unit(15, "cm"), 
    barheight = unit(1, "cm")))+
    colorspace::scale_color_continuous_divergingx(
        palette = "RdYlBu",
        name = "Salary Quantiles",
        breaks = ~ quantile(.x, na.rm=T))+
    labs(
        y = "X1: Number of hits in the previous season",
        x = "X2: Number of years in the league",
        title = "Predictor Space"
    )+
    theme_minimal() +
    theme(
        plot.title.position = "plot",
        legend.position = "top",
        legend.title.position = "top",
        legend.direction = "horizontal"
    )

```

# Model output - Predictor Space

![from ISLR](images/hitters-prercitor-space-model.png)

#  Model output - Tree

![from ISLR](images/tree-simple-hitters.png)

# Region representation

<br><br>

[$R1 = \left\{X|Years<4.5\right\}$]{.fragment fragment-index='1'}
<br><br>
[$R2 = \left\{X|Years>=4.5,Hits<117.5\right\}$]{.fragment fragment-index='2'}
<br><br>
[$R2 = \left\{X|Years>=4.5,Hits>=117.5\right\}$]{.fragment fragment-index='3'}

# Tree terminology

![from ISLR](images/tree-simple-hitters-terminology.png)

# How *should* we carry out segmenting of predictor space?

# Segmenting - Theory

[Predictor Space of $p$ variables needs to be segmented into $J$ different regions.]{.fragment fragment-index='1'}
<br><br>
[**In theory**, the regions can be of any shape, however high-dimensional rectangles are chosen in practice for computational ease and interpretability]{.fragment fragment-index='2'}
<br><br>
[For every observation in region $R_j$, we make the same prediction. Mean or mode of the training observations in region $R_j$]{.fragment fragment-index='3'}
<br><br>
[Minimize: $\sum_{j=1}^{J}{\sum_{i \in R_j}{(y_i - \hat{y}_{R_j})^2}}$]{.fragment fragment-index='4'} 

# But 

[Not easy to consider all possible cutpoints for all possible predictors with all possible sequences]{.fragment fragment-index='1'}
<br><br>
[We use high-dimensional rectangles instead of any shape for ease of interpretation.]{.fragment fragment-index='2'}

# So, use Recursive Binary splitting

[*top-down*]{.fragment fragment-index='1'}
<br><br>
[**greedy**]{.fragment fragment-index='2'}

# top-down

<br><br>

> We brgin at the point where all observations are part of the same region. Hence the name *top-down*

# Greedy

<br><br>

> *Best split at a particular step.* We do not care about the future. A predictor $p$ and a cutpoint $s$ is chosen based on which split will lead to the lowest RSS.

This is carried out recursively, over and over again.

# Formally

We aim to minimize the following at every step
<br><br>


> $\sum_{i:x_i \in R_1 (j,s)}{(y_i - \hat{y}_{R_1})^2} + \sum_{i:x_i \in R_2 (j,s)}{(y_i - \hat{y}_{R_2})^2}$

# Fitting Regresison Trees

![from {palmerpenguins} website](https://allisonhorst.github.io/palmerpenguins/articles/examples_files/figure-html/unnamed-chunk-7-1.png)

# Fitting Regresison Trees

```{r}
#| echo: true

tree(body_mass_g ~ ., data = penguins) -> peng_mass_tree

peng_mass_tree
```

# Fitting Regression Trees

```{r}
#| echo: true

peng_mass_tree$frame
```

# Fitting Regression Trees

```{r}
#| echo: true

peng_mass_tree$where
```

# Fitting Regression Trees


```{r}
#| echo: true

summary(peng_mass_tree)
```

# Fitting Regression Trees


```{r}
#| echo: true

plot(peng_mass_tree)
text(peng_mass_tree, pretty = 0)

```

# Fitting Regression Trees


```{r}
#| echo: true

na.omit(penguins) |>
    mutate(
        pred_mass = predict(peng_mass_tree)
    ) |>
        relocate(body_mass_g, pred_mass, everything())
```

# Do it yourself

<br>
Run the linear model using the penguins data:
<br>
$bodymass = species + sex$
<br>
compare the RSS for the linear model with the RSS of the decision tree.

# Cant just keep splitting

<br>
some terminal nodes have very few observations
<br>

```{r}
tree(Salary ~ ., data = Hitters) -> sal_hit_tree

sal_hit_tree
```

# Cant just keep splitting

```{r}
#| echo: false
tree(Salary ~ ., data = Hitters) -> sal_hit_tree

plot(sal_hit_tree)
text(sal_hit_tree, pretty = 0)

```

# So, when to stop
<br>
[The recursive splitting approach is likely to overfit]{.fragment fragment-index='1'}
<br><br>
[Leads to a complex tree]{.fragment fragment-index='2'}
<br><br>
[A realtively smaller tree can avoid overfitting via lower variance. Providing better interpretation at the cost of little bias]{.fragment fragment-index='3'}
<br><br>
[One could say that we split only if reduction in RSS is larger than some value. But this can be short sighted]{.fragment fragment-index='4'}

# So what to do
<br>
[Grow the full Tree.]{.fragment fragment-index='1'}
<br><br>
[Prune it back to a smaller subtree]{.fragment fragment-index='2'}
<br><br>
[*But which is the best subtree*]{.fragment fragment-index='3'}
<br><br>
[Well, intuitively, one that leads to the lowest test error rate]{.fragment fragment-index='4'}

# Finding the best subtree

<br>
[We can use cross-validation to estimate test error rate]{.fragment fragment-index='1'}
<br><br>
[But there can be so many subtrees]{.fragment fragment-index='2'}
<br><br>
[So, we can select some small number of trees using cost complexity pruning or weakest link pruning]{.fragment fragment-index='3'}

# Cost Complexity Pruning

<br><br>

$\sum_{m=1}^{|T|}{\sum_{i:x_i \in R_m}{(y_i - \hat{y}_{R_m})^2}} + \alpha |T|$

# The Combined Algorithm

1. Use Recursive binary splitting to grow a large tree.
2. Apply cost complexity pruning to get a sequence of best subtrees, as a function of $\alpha$
3. Use K-fold CV to choose $\alpha$.
    1. Repeat steps 1 and 2 on all but the Kth fold of training data
    2. Evaluate the mse on the left-out Kth fold, as a fucntion of $\alpha$. Average results of each value of $\alpha$, choose ove that minimizes average error.
4. Return the subtree from step 2 that corresponds with the value of chosen $\alpha$

# Step 1 - Grow the full tree

```{r}
#| echo: true

rsample::initial_split(data = Hitters,prop = .75) -> hit_split
rsample::training(hit_split) -> train_tree
rsample::testing(hit_split) -> test_tree

tree(Salary ~ ., data = train_tree) -> sal_hit_tree
```

# Step 2 - Apply Cost Complexity Pruning


```{r}
#| echo: true

prune.tree(sal_hit_tree)
```

# Step 3  - Apply CV to choose $\alpha$

```{r}
#| echo: true

cv.tree(sal_hit_tree,FUN = prune.tree) -> cv_pruned_estimates

cv_pruned_estimates
```

# Step 3 - Apply CV to choose $\alpha$

```{r}
#| echo: true
#| output-location: slide

tibble(
    leaves = cv_pruned_estimates$size,
    rss = cv_pruned_estimates$dev
) |>
    ggplot(aes(leaves, rss)) +
    geom_point()+
    geom_line()+
    theme_minimal()
```


# Step 4 - get the subtree from the large tree


```{r}
#| echo: true

prune.tree(sal_hit_tree, best = 3) -> best_sub_tree_sal

plot(best_sub_tree_sal)
text(best_sub_tree_sal, pretty = 0)
```

# Lets predict using the model
```{r}
#| echo: true

predict(best_sub_tree_sal, test_tree)
```

# Do it yourself

For the `Boston` data set from {ISLR2}, run a decision tree model that estimates the median value of owner-occupied homes in $1000. Complete the full algorith steps of growing a larger tree and choosing an alpha for the best possible subtree.

# Classification Trees

[Response is qualitative]{.fragment fragment-index='1'}
<br><br>
[For making predictions, instead of using the *mean* of the training observations in a region, the *most commonly occuring* class of training observations in a region is used.]{.fragment fragment-index='2'}
<br><br>
[Classification trees are grown in a similar fashion to regression trees]{.fragment fragment-index='3'}
[But, instead of RSS we use the *classification error rate*]{.fragment fragment-index='4'}
<br><br>
[Classification Error Rate (E): The fraction of training observations in a region that do not belong to the most common class.]{.fragment fragment-index='5'}

# Classification Error Rate

<br>
$$E = 1- max(\hat{p}_{mk}) $$

Here $\hat{p}_{mk}$ is the proportion of training observations in the mth region of class k.
<br><br>
But, this is not *sufficiently sensitive* for tree growing, so we use other measures in practice.

# Classification Error Rate - Issues

:::: {.columns}
::: {.column}

```{mermaid}
%%| fig-height: 10
%%| fig-width: 6

flowchart 
    A[A50,B50] --> B[A10,B40]
    A[A50,B50] --> C[A40,B10]

```

:::
::: {.column}
```{mermaid}
%%| fig-height: 10
%%| fig-width: 6

flowchart 
    A[A50,B50] --> B[A50,B20]
    A[A50,B50] --> C[A0,B30]

```

:::
::::
<!-- end columns -->

# Classification Error Rate - Issues

[Does not favour "pure" nodes]{.fragment fragment-index='1'}
<br><br>
[Only cares about the *most common class*, ignores *by how much*]{.fragment fragment-index='2'}
<br><br>
[May result in premature stop in tree growing]{.fragment fragment-index='3'}
<br><br>
[Insensitive to minor changes in patterns]{.fragment fragment-index='4'}

# Alternative - Gini

<br><br>
$G = \sum_{k=1}^{K}{\hat{p}_{mk}(1-\hat{p}_{mk})}$
<br><br>
The Value of G will be small when node purity is high.

# Alternative - Entropy

<br><br>
$D = - \sum_{k=1}^{K}{\hat{p}_{mk}log\hat{p}_{mk}}$
<br><br>
The value of D will be near zero for $\hat{p}_{mk}$ near zero or one

# Classification Error Rate - Issues
```{r}
#| echo: true
#| output-location: slide

tibble::tibble(
  prop_a = seq(0,1,by = 0.001),
  prop_b = 1- prop_a,
  max = ifelse(prop_a>=prop_b, prop_a, prop_b),
  E =  1 - max,
  G = 2*(prop_a*prop_b),
  En = -1*((prop_a*log(prop_a))+(prop_b*log(prop_b)))
)  |> 
  ggplot2::ggplot(ggplot2::aes(prop_a,E))+
  ggplot2::geom_line() +
  ggplot2::geom_line(ggplot2::aes(y = G), colour = "green") +
  ggplot2::geom_line(ggplot2::aes(y = En), colour = "steelblue") +
  ggplot2::theme_minimal() +
  ggplot2::labs(
    x = "probablity of Class A at a given node",
    y = "Y",
    title = "Comparing Classificaiton Error Rate, Gini and Entropy",
    subtitle = "Toy example with two classes: A and B"
  )
```

# A comment on E, G and D

<br><br>
[Best to use G or D for splitting]{.fragment fragment-index='1'}
<br><br>
[For pruning and gauging overall accuracy E is preferable if prediction accuracy is the goal.]{.fragment fragment-index='2'}
<br><br>
[The {tree} takes care of all this by default]{.fragment fragment-index='3'}

# code refrence

```{r}
#| echo: true
#| eval: false

tree(league ~ . , data = Hitters) -> mod

prune(mod, method = "misclass")

cv.tree(mod, FUN = prune.misclass)


```

# Do it yourself

[This link](https://www.statlearning.com/s/Heart.csv) has the `Heart data` for paitents with chest pains. The variable `AHD` is a binary variable where `Yes` refers to existing heart disease. Create a decision tree model, a full grown tree, to understand factors for predicting `AHD`. Also, prune this tree by choosing the appropriate $\alpha$. Write an interpretation note for your model.
<br>
Does your final and full grown trees have some spits that estimate the same response for terminal nodes? Why does that happen?